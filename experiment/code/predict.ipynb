{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import os\n",
    "import glob\n",
    "from tensorflow.keras.utils import img_to_array\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "No file or directory found at ../model/keras/best_model.hdf5",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     model \u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mmodels\u001b[38;5;241m.\u001b[39mload_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../model/keras/best_model.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m----> 4\u001b[0m a \u001b[38;5;241m=\u001b[39m \u001b[43mgetmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m, in \u001b[0;36mgetmodel\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetmodel\u001b[39m():\n\u001b[1;32m----> 2\u001b[0m     model \u001b[38;5;241m=\u001b[39m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m../model/keras/best_model.hdf5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[1;32mc:\\Users\\B\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\B\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\saving\\save.py:226\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_str, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39mexists(filepath_str):\n\u001b[1;32m--> 226\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(\n\u001b[0;32m    227\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo file or directory found at \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfilepath_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    228\u001b[0m         )\n\u001b[0;32m    230\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mgfile\u001b[38;5;241m.\u001b[39misdir(filepath_str):\n\u001b[0;32m    231\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m saved_model_load\u001b[38;5;241m.\u001b[39mload(\n\u001b[0;32m    232\u001b[0m             filepath_str, \u001b[38;5;28mcompile\u001b[39m, options\n\u001b[0;32m    233\u001b[0m         )\n",
      "\u001b[1;31mOSError\u001b[0m: No file or directory found at ../model/keras/best_model.hdf5"
     ]
    }
   ],
   "source": [
    "def getmodel():\n",
    "    model =keras.models.load_model('../model/keras/best_model.hdf5')\n",
    "    return model\n",
    "a = getmodel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getmodel():\n",
    "    model =keras.models.load_model('../model/keras/best_model.hdf5')\n",
    "    return model\n",
    "\n",
    "def preprocess_frame(data,img_size):\n",
    "    data = imutils.resize(data, width=img_size)\n",
    "    grey = cv2.cvtColor(data,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    return data,grey\n",
    "\n",
    "def detectface(frame,scale=1.05, Neigh=11, mSize=(20, 20)):\n",
    "    detector = cv2.CascadeClassifier('../model/haarcascade/haarcascade_frontalface_default.xml')\n",
    "    return detector.detectMultiScale(frame, scaleFactor=scale, minNeighbors=Neigh, minSize=mSize,flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "\n",
    "def process_data(data,model,img_height=255,img_size=400):\n",
    "\n",
    "    EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "\n",
    "    frame,grey = preprocess_frame(data,img_size)    \n",
    "    frameClone = frame.copy()\n",
    "    canvas = np.zeros((img_height, img_size, 3), dtype=\"uint8\")\n",
    "    rects = detectface(grey)\n",
    "    if len(rects) > 0:\n",
    "\n",
    "        sort_rect = sorted(rects, reverse=False,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))\n",
    "\n",
    "        for o,rect in enumerate(sort_rect):        \n",
    "\n",
    "            (fX, fY, fW, fH) = rect \n",
    "\n",
    "            roi = grey[fY:fY + fH, fX:fX + fW]\n",
    "            roi = cv2.resize(roi, (48, 48))\n",
    "            roi = roi.astype(\"float\") / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "            preds = model.predict(roi)[0]\n",
    "            label = EMOTIONS[preds.argmax()]\n",
    "\n",
    "            if o == 0:\n",
    "                for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                    # construct the label text\n",
    "                    text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                    # draw the label + probability bar on the canvas\n",
    "                    w = int(prob * img_size)\n",
    "                    cv2.rectangle(canvas, (5, (i * 35) + 5),\n",
    "                    (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                    cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                    (255, 255, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            text_all = f'Face {o+1} {label} {preds.max()*100:.2f}%'\n",
    "\n",
    "            cv2.putText(frameClone, text_all, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    else:\n",
    "        label = 'Notfound'\n",
    "    \n",
    "    return frameClone,canvas,label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getdata(type,input_path=None,output_path=None):\n",
    "\n",
    "    img_size=400\n",
    "    img_height=255\n",
    "\n",
    "    model = getmodel()\n",
    "\n",
    "    # Input is an image\n",
    "    if type =='image' and input_path is not None:\n",
    "        path_list  = glob.glob(input_path + '/*.jpg')\n",
    "        if not(path_list):\n",
    "            print('No image found please look at the path and the image extention is it jpg exit...')\n",
    "            return\n",
    "    \n",
    "        for path in path_list:\n",
    "            data = cv2.imread(path)\n",
    "\n",
    "            pred_data,canvas,label = process_data(data,model,img_height,img_size)\n",
    "\n",
    "            cv2.imshow(\"Face\", pred_data)\n",
    "            cv2.imshow(\"Probabilities\", canvas)\n",
    "\n",
    "            cv2.waitKey(0) \n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            filename = path.split(os.path.sep)[-1]\n",
    "            filename = label + '_' + filename\n",
    "            output_path = output_path+ '//'+ filename\n",
    "            cv2.imwrite(filename=output_path,img=pred_data)\n",
    "            \n",
    "    \n",
    "    # Input is  video\n",
    "    else:\n",
    "\n",
    "        if type =='video' and input_path is not None:\n",
    "            path_list = glob.glob(input_path +'/*.mp4')\n",
    "            if not(path_list):\n",
    "                print('No video found please look at the path and the video extention is it mp4 exit...')\n",
    "                return\n",
    "            path = path_list[0]\n",
    "\n",
    "            filename = path.split(os.path.sep)[-1]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MP4V')  \n",
    "\n",
    "            output_path = output_path+ '//'+ f'output_{filename}'\n",
    "            output_video = cv2.VideoWriter(output_path, fourcc, 30.0, (img_size,225))            \n",
    "\n",
    "            camera = cv2.VideoCapture(path)\n",
    "\n",
    "        elif type =='live':\n",
    "            filename = 'live'\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'MP4V')  \n",
    "\n",
    "            output_path = output_path+ '//'+ f'output_{filename}.mp4'\n",
    "\n",
    "            output_video = cv2.VideoWriter(output_path, fourcc, 30.0, (img_size,300))\n",
    "\n",
    "            camera = cv2.VideoCapture(0)\n",
    "\n",
    "        else:\n",
    "            print('No path pass if mode is image or video please pass path directory...')\n",
    "            return \n",
    "\n",
    "        while True:\n",
    "            grabbed,data = camera.read()\n",
    "\n",
    "            if not grabbed:\n",
    "                print('End of Video...')\n",
    "                break\n",
    "\n",
    "            pred_data,canvas,label = process_data(data,model,img_height,img_size)\n",
    "\n",
    "            cv2.imshow(\"Face\", pred_data)            \n",
    "            cv2.imshow(\"Probabilities Of Face 1\", canvas)\n",
    "\n",
    "            output_video.write(pred_data)\n",
    "  \n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        camera.release()\n",
    "        output_video.release()\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_type = ['image','video','live']\n",
    "process_mode ={\n",
    "    'image' : [ '../data/image','../output/image'],\n",
    "    'video' : [ '../data/video','../output/video'],\n",
    "    'live'  : [None,'../output/video']\n",
    "}\n",
    "mode = process_type[0]\n",
    "\n",
    "getdata(mode,process_mode[mode][0],process_mode[mode][1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "model = keras.models.load_model('../model/keras/gender_model.hdf5')\n",
    "converter = lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "tflite_model_path = '../model/keras/gender_model.tflite'\n",
    "with open(tflite_model_path, 'wb+') as f:\n",
    "    f.write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import lite\n",
    "converter = lite.TFLiteConverter.from_keras_model(a)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to a file\n",
    "tflite_model_path = '../model/keras/best_model.tflite'\n",
    "with open(tflite_model_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "def load_tflite_model(model_path):\n",
    "    interpreter = lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "interpreter = load_tflite_model(tflite_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data,model,img_height=255,img_size=400):\n",
    "\n",
    "    EMOTIONS = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "\n",
    "    frame,grey = preprocess_frame(data,img_size)    \n",
    "    frameClone = frame.copy()\n",
    "    canvas = np.zeros((img_height, img_size, 3), dtype=\"uint8\")\n",
    "    rects = detectface(grey)\n",
    "    if len(rects) > 0:\n",
    "\n",
    "        sort_rect = sorted(rects, reverse=False,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))\n",
    "\n",
    "        for o,rect in enumerate(sort_rect):        \n",
    "\n",
    "            (fX, fY, fW, fH) = rect \n",
    "\n",
    "            roi = grey[fY:fY + fH, fX:fX + fW]\n",
    "            roi = cv2.resize(roi, (48, 48))\n",
    "            roi = roi.astype(\"float\") / 255.0\n",
    "            roi = img_to_array(roi)\n",
    "            roi = np.expand_dims(roi, axis=0)\n",
    "            preds = model.predict(roi)[0]\n",
    "            label = EMOTIONS[preds.argmax()]\n",
    "\n",
    "            if o == 0:\n",
    "                for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):\n",
    "                    # construct the label text\n",
    "                    text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                    # draw the label + probability bar on the canvas\n",
    "                    w = int(prob * img_size)\n",
    "                    cv2.rectangle(canvas, (5, (i * 35) + 5),\n",
    "                    (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                    cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                    (255, 255, 255), 2)\n",
    "            \n",
    "            text_all = f'Face {o+1} {label} {preds.max()*100:.2f}%'\n",
    "\n",
    "            cv2.putText(frameClone, text_all, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "            cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    else:\n",
    "        label = 'Notfound'\n",
    "    \n",
    "    return frameClone,canvas,label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size=400\n",
    "img_height=255\n",
    "input_path = '../data/image'\n",
    "model = getmodel()\n",
    "\n",
    "path_list  = glob.glob(input_path + '/*.jpg')\n",
    "path = path_list[0]\n",
    "\n",
    "\n",
    "\n",
    "data = cv2.imread(path)\n",
    "\n",
    "pred_data,canvas,label = process_data2(data,model,img_height,img_size)\n",
    "\n",
    "cv2.imshow(\"Face\", pred_data)\n",
    "cv2.imshow(\"Probabilities\", canvas)\n",
    "\n",
    "cv2.waitKey(0) \n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import imutils\n",
    "import os\n",
    "import glob\n",
    "import argparse\n",
    "from tensorflow import lite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i.title() for i in ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_tflite_model(model_path):\n",
    "    interpreter = lite.Interpreter(model_path=model_path)\n",
    "    interpreter.allocate_tensors()\n",
    "    return interpreter\n",
    "\n",
    "\n",
    "def inference_tflite_model(interpreter, input_data):\n",
    "    input_details = interpreter.get_input_details()\n",
    "    output_details = interpreter.get_output_details()\n",
    "\n",
    "    # Assuming the input shape is (batch_size, height, width, channels)\n",
    "    input_data = input_data.reshape(input_details[0]['shape'])\n",
    "\n",
    "    interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "    interpreter.invoke()\n",
    "\n",
    "    output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "    return output_data\n",
    "\n",
    "def img_toarray(img,type):   \n",
    "    x = np.asarray(img, dtype=\"float32\")\n",
    "    if type=='rgb':\n",
    "        x = x.reshape((x.shape[0], x.shape[1], 3))\n",
    "    else:\n",
    "        x = x.reshape((x.shape[0], x.shape[1], 1))\n",
    "\n",
    "    return np.expand_dims(x, axis=0)\n",
    "\n",
    "def preprocess_frame(data):\n",
    "    input_mood = cv2.resize(data, dsize=(48,48))\n",
    "    input_mood = cv2.cvtColor(input_mood,cv2.COLOR_BGR2GRAY).astype(\"float\") / 255.0\n",
    "    input_age = cv2.resize(data, dsize=(200,200)).astype(\"float\") / 255.0\n",
    "    input_gender = cv2.resize(data, dsize=(110,90)).astype(\"float\") / 255.0\n",
    "\n",
    "    input_mood = img_toarray(input_mood,'grey')\n",
    "    input_age = img_toarray(input_age,'rgb')\n",
    "    input_gender = img_toarray(input_gender,'rgb')\n",
    "\n",
    "\n",
    "    return input_mood,input_age,input_gender\n",
    "\n",
    "def detectface(model_path):\n",
    "    detector = cv2.CascadeClassifier(model_path)\n",
    "    return detector\n",
    "\n",
    "\n",
    "def process_data(data,img_height=255,img_size=400):\n",
    "\n",
    "    age_model = '../model/keras/age_model.tflite'\n",
    "    mood_model = '../model/keras/mood_model.tflite'\n",
    "    gender_model = '../model/keras/gender_model.tflite'\n",
    "    detector_path = '../model/haarcascade/haarcascade_frontalface_default.xml'\n",
    "\n",
    "    EMOTIONS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
    "    AGE = ['0-2', '13-20', '21-32', '33-48', '3-6', '49-53', '54+', '7-12']\n",
    "    GENDER = ['FEMALE','MALE']\n",
    "\n",
    "    # frame,grey = preprocess_frame(data,img_size,cv2.COLOR_BGR2GRAY)    \n",
    "    data = imutils.resize(data,img_size)\n",
    "    # frameClone = frame.copy()\n",
    "    canvas = np.zeros((img_height, img_size, 3), dtype=\"uint8\")\n",
    "    detector =detectface(detector_path)\n",
    "\n",
    "    rects = detector.detectMultiScale(data, \n",
    "                                      scaleFactor=1.05, minNeighbors=11, \n",
    "                                      minSize=(20, 20),flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "    if len(rects) > 0:\n",
    "        \n",
    "        \n",
    "\n",
    "        age_model = load_tflite_model(age_model)\n",
    "        gender_model = load_tflite_model(gender_model)\n",
    "        mood_model = load_tflite_model(mood_model)\n",
    "\n",
    "        # frame,input_mood,input_age,input_gender = preprocess_frame(data)    \n",
    "        frameClone = data.copy()\n",
    "        sort_rect = sorted(rects, reverse=False,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))\n",
    "\n",
    "        for o,rect in enumerate(sort_rect):        \n",
    "\n",
    "            (fX, fY, fW, fH) = rect \n",
    "\n",
    "            roi = data[fY:fY + fH, fX:fX + fW]\n",
    "            \n",
    "            input_mood,input_age,input_gender = preprocess_frame(roi)\n",
    "\n",
    "            # roi = cv2.resize(roi, (48, 48))\n",
    "            # roi = roi.astype(\"float\") / 255.0\n",
    "            # roi = img_toarray(roi)\n",
    "            # roi = np.expand_dims(roi, axis=0)\n",
    "            # preds = model.predict(roi)[0]\n",
    "\n",
    "            preds_mood = inference_tflite_model(mood_model,input_mood)[0]\n",
    "            preds_age = inference_tflite_model(age_model,input_age)[0]\n",
    "            # preds_gender = inference_tflite_model(gender_model,input_gender)[0]\n",
    "            \n",
    "            label = EMOTIONS[preds_mood.argmax()]\n",
    "            label_age = AGE[preds_age.argmax()]\n",
    "            # label_gender = GENDER[preds_gender.argmax()]\n",
    "\n",
    "            if o == 0:\n",
    "                for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds_mood)):\n",
    "                    # construct the label text\n",
    "                    text = \"{}: {:.2f}%\".format(emotion, prob * 100)\n",
    "\n",
    "                    # draw the label + probability bar on the canvas\n",
    "                    w = int(prob * img_size)\n",
    "                    cv2.rectangle(canvas, (5, (i * 35) + 5),\n",
    "                    (w, (i * 35) + 35), (0, 0, 255), -1)\n",
    "                    cv2.putText(canvas, text, (10, (i * 35) + 23),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45,\n",
    "                    (255, 255, 255), 1, cv2.LINE_AA)\n",
    "            \n",
    "            text_all = f'Face {o+1} {label} {preds_mood.max()*100:.2f}% {label_age}'\n",
    "            pos = o+1\n",
    "            # text_all = f'Face {pos} {label} {label_age}'\n",
    "\n",
    "            if (o % 2) == 0:\n",
    "                cv2.putText(frameClone, text_all, (fX, fY - (pos*10)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            else:\n",
    "                cv2.putText(frameClone, text_all, (fX, fY + fH + (pos*10)), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 1, cv2.LINE_AA)\n",
    "            cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)\n",
    "\n",
    "\n",
    "    else:\n",
    "        label = 'Notfound'\n",
    "        frameClone = data\n",
    "    \n",
    "    return frameClone,canvas,label\n",
    "    \n",
    "\n",
    "def getdata(type,input_path=None,output_path=None):\n",
    "\n",
    "    img_size=400\n",
    "    img_height=255\n",
    "\n",
    "    # model = getmodel(model_path)\n",
    "\n",
    "    # Input is an image\n",
    "    if type =='image' and input_path is not None:\n",
    "        path_list  = glob.glob(input_path + '/*.jpg')\n",
    "        if not(path_list):\n",
    "            print('No image found please look at the path and the image extention is it jpg exit...')\n",
    "            return\n",
    "    \n",
    "        for path in path_list:\n",
    "            data = cv2.imread(path)\n",
    "\n",
    "            pred_data,canvas,label = process_data(data,img_height,img_size)\n",
    "\n",
    "            cv2.imshow(\"Face\", pred_data)\n",
    "            cv2.imshow(\"Probabilities\", canvas)\n",
    "\n",
    "            cv2.waitKey(0) \n",
    "            cv2.destroyAllWindows()\n",
    "\n",
    "            filename = path.split(os.path.sep)[-1]\n",
    "            filename = label + '_' + filename\n",
    "            output_path = output_path+ '//'+ filename\n",
    "            cv2.imwrite(filename=output_path,img=pred_data)\n",
    "            \n",
    "    \n",
    "    # Input is  video\n",
    "    else:\n",
    "\n",
    "        if type =='video' and input_path is not None:\n",
    "            path_list = glob.glob(input_path +'/*.mp4')\n",
    "            if not(path_list):\n",
    "                print('No video found please look at the path and the video extention is it mp4 exit...')\n",
    "                return\n",
    "            path = path_list[0]\n",
    "\n",
    "            filename = path.split(os.path.sep)[-1]\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "\n",
    "            output_path = output_path+ '//'+ f'output_{filename}'\n",
    "            output_video = cv2.VideoWriter(output_path, fourcc, 60.0, (img_size,225))            \n",
    "\n",
    "            camera = cv2.VideoCapture(path)\n",
    "\n",
    "        elif type =='live':\n",
    "            filename = 'live'\n",
    "            fourcc = cv2.VideoWriter_fourcc(*'mp4v')  \n",
    "\n",
    "            output_path = output_path+ '//'+ f'output_{filename}.mp4'\n",
    "\n",
    "            output_video = cv2.VideoWriter(output_path, fourcc, 60.0, (img_size,300))\n",
    "\n",
    "            camera = cv2.VideoCapture(0)\n",
    "\n",
    "        else:\n",
    "            print('No path pass if mode is image or video please pass path directory...')\n",
    "            return \n",
    "\n",
    "        while True:\n",
    "            grabbed,data = camera.read()\n",
    "\n",
    "            if not grabbed:\n",
    "                print('End of Video...')\n",
    "                break\n",
    "\n",
    "            pred_data,canvas,label = process_data(data,img_height,img_size)\n",
    "\n",
    "            cv2.imshow(\"Face\", pred_data)            \n",
    "            cv2.imshow(\"Probabilities Of Face 1\", canvas)\n",
    "\n",
    "            output_video.write(pred_data)\n",
    "  \n",
    "            \n",
    "            if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                break\n",
    "\n",
    "        camera.release()\n",
    "        output_video.release()\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        print('Script finish...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script finish...\n"
     ]
    }
   ],
   "source": [
    "a = getdata('live',None,'../output/video')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
